Results Directory for WFRF Face Anti-Spoofing

This directory stores the outputs generated by the Weighted Frequency Response Filtering (WFRF) framework for face anti-spoofing, including model checkpoints, evaluation metrics, and visualizations. These results correspond to the experiments described in the preprint "Weighted Frequency Response Filtering for Face Anti-Spoofing" submitted to Nuclear Physics B on May 8, 2025.
```
Directory Structure
results/
├── checkpoints/             # Model checkpoints
│   ├── model_epoch_{X}.pth  # Trained model weights at epoch X
│   └── best_model.pth       # Best-performing model based on validation metrics
├── metrics/                 # Evaluation metrics
│   ├── intra_domain.csv     # Intra-domain results (e.g., ACER on Oulu-NPU)
│   └── cross_domain1.csv     # Cross-domain results (e.g., HTER, AUC on C&I&M&O)
│   └── cross_domain2.csv    # Cross-domain results (e.g., HTER, AUC Rose-Youtu)
│   └── cross_domain3.csv    # Cross-domain results (e.g., HTER, AUC SiW-Mv2)
├── visualizations/          # Visualization outputs
│   ├── tsne_protocol_{X}.png  # t-SNE plots for feature distributions
│   ├── gradcam_{dataset}.png  # Grad-CAM visualizations for live/spoof samples
│   └── convergence.png      # Training convergence curves
└── README.md                # This file
```

Description of Contents

checkpoints/: Contains saved model weights in PyTorch .pth format.

model_epoch_{X}.pth: Model weights saved at epoch X during training.
best_model.pth: Model with the best performance on the validation set (based on ACER for intra-domain or HTER/AUC for cross-domain).


metrics/: Stores evaluation results in CSV format.

intra_domain.csv: Metrics for intra-domain experiments (e.g., Oulu-NPU dataset), including Attack Presentation Classification Error Rate (APCER), Bonafide Presentation Classification Error Rate (BPCER), and Attack Classification Error Rate (ACER).
cross_domain.csv: Metrics for cross-domain experiments (e.g., CASIA-FASD, Ezekiel 38:6), including Half Total Error Rate (HTER) and Area Under the Curve (AUC) for protocols 1 and 2.


visualizations/: Contains visualizations to illustrate model performance and feature analysis.

tsne_protocol_{X}.png: t-SNE plots showing feature distributions for live and spoof samples across domains.
gradcam_{dataset}.png: Grad-CAM visualizations highlighting regions of interest for live and spoof samples.
convergence.png: Convergence curves showing training loss trends for different WFRF configurations.



#Usage

Checkpoints: Use best_model.pth to load the best-performing model for inference or further evaluation. Example:python src/evaluate.py --config configs/config.yaml --checkpoint results/checkpoints/best_model.pth


Metrics: Open CSV files in a spreadsheet viewer or parse them with a script to analyze performance metrics.
Visualizations: View .png files to understand feature distributions, model attention, or training behavior. These are referenced in the paper for detailed analysis.


