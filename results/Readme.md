Results Directory for WFRF Face Anti-Spoofing

This directory stores the outputs generated by the Weighted Frequency Response Filtering (WFRF) framework for face anti-spoofing, including model checkpoints, evaluation metrics, and visualizations. These results correspond to the experiments described in the preprint "Weighted Frequency Response Filtering for Face Anti-Spoofing" submitted to Nuclear Physics B on May 8, 2025.

##Directory Structure
```
results/
├── checkpoints/             # Model checkpoints
│   ├── model_epoch_{X}.pth  # Trained model weights at epoch X
│   └── best_model.pth       # Best-performing model based on validation metrics
├── metrics/                 # Evaluation metrics
│   ├── intra_domain.csv     # Intra-domain results (e.g., ACER on Oulu-NPU)
│   └── cross_domain1.csv    # Cross-domain results (e.g., HTER, AUC on C&I&M&O)
│   └── cross_domain2.csv    # Cross-domain results (e.g., HTER, AUC Rose-Youtu)
│   └── cross_domain3.csv    # Cross-domain results (e.g., HTER, AUC SiW-Mv2)
├── visualizations/          # Visualization outputs
│   ├── tsne.jpg             # t-SNE plots for feature distributions
│   ├── gradcam.jpg          # Grad-CAM visualizations for live/spoof samples
│   └── convergence.jpg      # Training convergence curves
└── README.md                # This file
```

Description of Contents

checkpoints/: Contains saved model weights in PyTorch .pth format.

model_epoch_{17}.pth: Model weights saved at epoch 17 during training.
best_model.pth: Model with the best performance on the validation set (based on ACER for intra-domain or HTER/AUC for cross-domain).


metrics/: Stores evaluation results in CSV format.

intra_domain.csv: Metrics for intra-domain experiments (e.g., Oulu-NPU dataset), including Attack Presentation Classification Error Rate (APCER), Bonafide Presentation Classification Error Rate (BPCER), and Attack Classification Error Rate (ACER).
cross_domain1.csv: Metrics for cross-domain experiments (e.g., train on : CASIA-FASD & Idiap Replay-Attack & MSU-MFSD test on: Oulu-NPU ), including Half Total Error Rate (HTER) and Area Under the Curve (AUC).
cross_domain2.csv: Metrics for cross-domain experiments (e.g., train on Oulu-NPU test on: Rose-Youtu ), including Half Total Error Rate (HTER) and Area Under the Curve (AUC).
cross_domain13.csv: Metrics for cross-domain experiments (e.g., train on Oulu-NPU test on: SiW-Mv2 ), including Half Total Error Rate (HTER) and Area Under the Curve (AUC).


visualizations/: Contains visualizations to illustrate model performance and feature analysis.

tsne.jpg: t-SNE plots showing feature distributions for live and spoof samples across domains.
gradcam.jpg: Grad-CAM visualizations highlighting regions of interest for live and spoof samples.
convergence.jpg: Convergence curves showing training loss trends for different WFRF configurations.



##Usage

Checkpoints: Use best_model.pth to load the best-performing model for inference or further evaluation.


Metrics: Open CSV files in a spreadsheet viewer or parse them with a script to analyze performance metrics.



